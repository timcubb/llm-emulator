{
  "name": "llm-emulator",
  "version": "0.4.1",
  "description": "Enterprise-grade LLM mock server for local and CI: scenarios, faults, latency, contracts, VCR. Supports standalone server and Express middleware.",
  "type": "module",
  "bin": {
    "llm-emulator": "./bin/llm-emulator.js"
  },
  "exports": {
    ".": {
      "import": "./src/index.js"
    }
  },
  "scripts": {
    "start": "node ./bin/llm-emulator.js ./examples/config.mjs",
    "dev": "node ./bin/llm-emulator.js ./examples/config.mjs --env local --scenario checkout"
  },
  "keywords": [
    "llm",
    "mock",
    "testing",
    "openai",
    "anthropic",
    "gemini",
    "ollama",
    "scenarios",
    "faults",
    "vcr",
    "middleware",
    "express"
  ],
  "author": "Your Name",
  "license": "MIT",
  "engines": {
    "node": ">=18.17"
  },
  "dependencies": {
    "@google/genai": "^1.29.0",
    "@xenova/transformers": "^2.17.2",
    "ajv": "^8.17.1",
    "cors": "^2.8.5",
    "express": "^4.19.2",
    "minimist": "^1.2.8",
    "openai": "^6.8.1",
    "uuid": "^9.0.1"
  }
}
